{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uIQ2VYSgGqo9",
        "outputId": "af83eef3-b31d-42b7-c5a5-ac29f5f8f7a2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://download.pytorch.org/whl/cu118\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (0.23.0+cu126)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision) (11.3.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (4.0.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets) (3.20.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from datasets) (2.32.4)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.12/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets) (3.6.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.35.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from datasets) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from datasets) (6.0.3)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.13.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24.0->datasets) (1.1.10)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (2025.10.5)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.22.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Collecting torchmetrics\n",
            "  Downloading torchmetrics-1.8.2-py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: numpy>1.20.0 in /usr/local/lib/python3.12/dist-packages (from torchmetrics) (2.0.2)\n",
            "Requirement already satisfied: packaging>17.1 in /usr/local/lib/python3.12/dist-packages (from torchmetrics) (25.0)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from torchmetrics) (2.8.0+cu126)\n",
            "Collecting lightning-utilities>=0.8.0 (from torchmetrics)\n",
            "  Downloading lightning_utilities-0.15.2-py3-none-any.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (75.2.0)\n",
            "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.12/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (4.15.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (3.20.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (3.4.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.0.0->torchmetrics) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.0.0->torchmetrics) (3.0.3)\n",
            "Downloading torchmetrics-1.8.2-py3-none-any.whl (983 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m983.2/983.2 kB\u001b[0m \u001b[31m59.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lightning_utilities-0.15.2-py3-none-any.whl (29 kB)\n",
            "Installing collected packages: lightning-utilities, torchmetrics\n",
            "Successfully installed lightning-utilities-0.15.2 torchmetrics-1.8.2\n"
          ]
        }
      ],
      "source": [
        "!pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
        "!pip install datasets\n",
        "!pip install torchmetrics"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing libraries\n",
        "\n",
        "# PyTorch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from datasets import load_dataset\n",
        "import torchmetrics\n",
        "\n",
        "# Math\n",
        "import math\n",
        "\n",
        "# HuggingFace libraries\n",
        "from tokenizers import Tokenizer\n",
        "from tokenizers.models import WordLevel\n",
        "from tokenizers.trainers import WordLevelTrainer\n",
        "from tokenizers.pre_tokenizers import Whitespace\n",
        "\n",
        "# Pathlib\n",
        "from pathlib import Path\n",
        "\n",
        "# typing\n",
        "from typing import Any\n",
        "\n",
        "# Library for progress bars in loops\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Importing library of warnings\n",
        "import warnings"
      ],
      "metadata": {
        "id": "dReImB5iG1Dp"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class InputEmbeddings(nn.Module):\n",
        "    def __init__(self, d_model: int, vocab_size: int) -> None:\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.embedding(x) * math.sqrt(self.d_model)\n"
      ],
      "metadata": {
        "id": "VlMPEWrDHUlj"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model: int, seq_len: int, dropout: float) -> None:\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.seq_len = seq_len\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        # ایجاد ماتریس PE با اندازه (seq_len, d_model)\n",
        "        pe = torch.zeros(seq_len, d_model)\n",
        "        position = torch.arange(0, seq_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "\n",
        "        # اعمال توابع سینوس و کسینوس\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0)  # (1, seq_len, d_model)\n",
        "\n",
        "        # ذخیره در بافر تا در GPU هم منتقل بشه ولی یادگیری نداشته باشه\n",
        "        self.register_buffer(\"pe\", pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        x: (batch_size, seq_len, d_model)\n",
        "        \"\"\"\n",
        "        # جمع Embedding با PE و اعمال Dropout\n",
        "        x = x + self.pe[:, :x.size(1), :].to(x.device)\n",
        "        return self.dropout(x)\n"
      ],
      "metadata": {
        "id": "EKMB2SirDOeu"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LayerNormalization(nn.Module):\n",
        "\n",
        "    def __init__(self, features: int, eps:float=10**-6) -> None:\n",
        "        super().__init__()\n",
        "        self.eps = eps\n",
        "        self.alpha = nn.Parameter(torch.ones(features))\n",
        "        self.bias = nn.Parameter(torch.zeros(features))\n",
        "\n",
        "    def forward(self, x):\n",
        "        mean = x.mean(dim = -1, keepdim = True)\n",
        "        std = x.std(dim = -1, keepdim = True)\n",
        "        return self.alpha * (x - mean) / (std + self.eps) + self.bias"
      ],
      "metadata": {
        "id": "xsMZvaTsIQl7"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForwardBlock(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model: int, d_ff: int, dropout: float) -> None:\n",
        "        super().__init__()\n",
        "        self.linear_1 = nn.Linear(d_model, d_ff)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.linear_2 = nn.Linear(d_ff, d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.linear_2(self.dropout(torch.relu(self.linear_1(x))))"
      ],
      "metadata": {
        "id": "JZI9tBruIY20"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttentionBlock(nn.Module):\n",
        "    def __init__(self, d_model: int, h: int, dropout: float) -> None:\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.h = h\n",
        "        assert d_model % h == 0, \"d_model is not divisible by h\"\n",
        "\n",
        "        self.d_k = d_model // h\n",
        "        self.w_q = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.w_k = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.w_v = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.w_o = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    @staticmethod\n",
        "    def attention(query, key, value, mask, dropout: nn.Dropout):\n",
        "        # query,key,value: (B, H, S, d_k)\n",
        "        scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(query.size(-1))  # (B,H,S,S)\n",
        "        if mask is not None:\n",
        "            scores = scores.masked_fill(mask == 0, float(\"-inf\"))  # masked positions -> -inf\n",
        "        attn = torch.softmax(scores, dim=-1)  # (B,H,S,S)\n",
        "        if dropout is not None:\n",
        "            attn = dropout(attn)\n",
        "        return torch.matmul(attn, value)  # (B,H,S,d_k)\n",
        "\n",
        "    def forward(self, q, k, v, mask=None):\n",
        "        B = q.size(0)\n",
        "\n",
        "        # Linear projections\n",
        "        Q = self.w_q(q)  # (B,S_q,d_model)\n",
        "        K = self.w_k(k)  # (B,S_k,d_model)\n",
        "        V = self.w_v(v)  # (B,S_k,d_model)\n",
        "\n",
        "        # Split into heads: (B,S, H*d_k) -> (B,H,S,d_k)\n",
        "        def split_heads(x):\n",
        "            return x.view(B, -1, self.h, self.d_k).transpose(1, 2).contiguous()\n",
        "\n",
        "        Q = split_heads(Q)\n",
        "        K = split_heads(K)\n",
        "        V = split_heads(V)\n",
        "\n",
        "        # Ensure mask shape is broadcastable to (B,H,S_q,S_k)\n",
        "        if mask is not None and mask.dim() == 3:\n",
        "            mask = mask.unsqueeze(1)  # (B,1,S_q,S_k)\n",
        "\n",
        "        # Scaled dot-product attention\n",
        "        x = MultiHeadAttentionBlock.attention(Q, K, V, mask, self.dropout)  # (B,H,S_q,d_k)\n",
        "\n",
        "        # Concatenate heads: (B,H,S,d_k) -> (B,S,H*d_k=d_model)\n",
        "        x = x.transpose(1, 2).contiguous().view(B, -1, self.h * self.d_k)\n",
        "\n",
        "        # Final linear projection\n",
        "        return self.w_o(x)\n"
      ],
      "metadata": {
        "id": "HOtBMBK8JD1_"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ResidualConnection(nn.Module):\n",
        "\n",
        "        def __init__(self, features: int, dropout: float) -> None:\n",
        "            super().__init__()\n",
        "            self.dropout = nn.Dropout(dropout)\n",
        "            self.norm = LayerNormalization(features)\n",
        "\n",
        "        def forward(self, x, sublayer):\n",
        "            return x + self.dropout(sublayer(self.norm(x)))"
      ],
      "metadata": {
        "id": "XNg1SPSiJIEq"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderBlock(nn.Module):\n",
        "\n",
        "    def __init__(self, features: int, self_attention_block: MultiHeadAttentionBlock, feed_forward_block: FeedForwardBlock, dropout: float) -> None:\n",
        "        super().__init__()\n",
        "        self.self_attention_block = self_attention_block\n",
        "        self.feed_forward_block = feed_forward_block\n",
        "        self.residual_connections = nn.ModuleList([ResidualConnection(features, dropout) for _ in range(2)])\n",
        "\n",
        "    def forward(self, x, src_mask):\n",
        "        x = self.residual_connections[0](x, lambda x: self.self_attention_block(x, x, x, src_mask))\n",
        "        x = self.residual_connections[1](x, self.feed_forward_block)\n",
        "        return x"
      ],
      "metadata": {
        "id": "I6fL0mvMJP8D"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "\n",
        "    def __init__(self, features: int, layers: nn.ModuleList) -> None:\n",
        "        super().__init__()\n",
        "        self.layers = layers\n",
        "        self.norm = LayerNormalization(features)\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, mask)\n",
        "        return self.norm(x)"
      ],
      "metadata": {
        "id": "bHZk9H5IJSp1"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderBlock(nn.Module):\n",
        "\n",
        "    def __init__(self, features: int, self_attention_block: MultiHeadAttentionBlock,\n",
        "                 cross_attention_block: MultiHeadAttentionBlock,\n",
        "                 feed_forward_block: FeedForwardBlock, dropout: float) -> None:\n",
        "        super().__init__()\n",
        "        self.self_attention_block = self_attention_block\n",
        "        self.cross_attention_block = cross_attention_block\n",
        "        self.feed_forward_block = feed_forward_block\n",
        "        self.residual_connections = nn.ModuleList([ResidualConnection(features, dropout) for _ in range(3)])\n",
        "\n",
        "    def forward(self, x, encoder_output, src_mask, tgt_mask):\n",
        "        x = self.residual_connections[0](x, lambda x: self.self_attention_block(x, x, x, tgt_mask))\n",
        "        x = self.residual_connections[1](x, lambda x: self.cross_attention_block(x, encoder_output, encoder_output, src_mask))\n",
        "        x = self.residual_connections[2](x, self.feed_forward_block)\n",
        "        return x"
      ],
      "metadata": {
        "id": "Gkrbn5pIJZSu"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(nn.Module):\n",
        "\n",
        "    def __init__(self, features: int, layers: nn.ModuleList) -> None:\n",
        "        super().__init__()\n",
        "        self.layers = layers\n",
        "        self.norm = LayerNormalization(features)\n",
        "\n",
        "    def forward(self, x, encoder_output, src_mask, tgt_mask):\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, encoder_output, src_mask, tgt_mask)\n",
        "        return self.norm(x)"
      ],
      "metadata": {
        "id": "NTXYG2WsJbgd"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ProjectionLayer(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model, vocab_size) -> None:\n",
        "        super().__init__()\n",
        "        self.proj = nn.Linear(d_model, vocab_size)\n",
        "\n",
        "    def forward(self, x) -> None:\n",
        "        return self.proj(x)"
      ],
      "metadata": {
        "id": "oaGAx26uBqVY"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer(nn.Module):\n",
        "    def __init__(self, encoder: Encoder, decoder: Decoder, src_embed: InputEmbeddings,\n",
        "                 tgt_embed: InputEmbeddings, src_pos: PositionalEncoding,\n",
        "                 tgt_pos: PositionalEncoding, projection_layer: ProjectionLayer) -> None:\n",
        "        super().__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.src_embed = src_embed\n",
        "        self.tgt_embed = tgt_embed\n",
        "        self.src_pos = src_pos\n",
        "        self.tgt_pos = tgt_pos\n",
        "        self.projection_layer = projection_layer\n",
        "\n",
        "    def encode(self, src, src_mask):\n",
        "        x = self.src_embed(src)     # (B, S_src, d_model)\n",
        "        x = self.src_pos(x)         # add positional encodings\n",
        "        return self.encoder(x, src_mask)\n",
        "\n",
        "    def decode(self, encoder_output: torch.Tensor, src_mask: torch.Tensor,\n",
        "               tgt: torch.Tensor, tgt_mask: torch.Tensor):\n",
        "        x = self.tgt_embed(tgt)     # (B, S_tgt, d_model)\n",
        "        x = self.tgt_pos(x)         # add positional encodings\n",
        "        return self.decoder(x, encoder_output, src_mask, tgt_mask)\n",
        "\n",
        "    def project(self, x):\n",
        "        return self.projection_layer(x)\n"
      ],
      "metadata": {
        "id": "OhqcTalNBuVP"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_transformer(src_vocab_size: int, tgt_vocab_size: int, src_seq_len: int,\n",
        "                      tgt_seq_len: int, d_model: int=512, N: int=6, h: int=8,\n",
        "                      dropout: float=0.1, d_ff: int=2048) -> Transformer:\n",
        "    src_embed = InputEmbeddings(d_model, src_vocab_size)\n",
        "    tgt_embed = InputEmbeddings(d_model, tgt_vocab_size)\n",
        "\n",
        "    src_pos = PositionalEncoding(d_model, src_seq_len, dropout)\n",
        "    tgt_pos = PositionalEncoding(d_model, tgt_seq_len, dropout)\n",
        "\n",
        "    encoder_blocks = []\n",
        "    for _ in range(N):\n",
        "        encoder_self_attention_block = MultiHeadAttentionBlock(d_model, h, dropout)\n",
        "        feed_forward_block = FeedForwardBlock(d_model, d_ff, dropout)\n",
        "        encoder_block = EncoderBlock(d_model, encoder_self_attention_block, feed_forward_block, dropout)\n",
        "        encoder_blocks.append(encoder_block)\n",
        "\n",
        "    decoder_blocks = []\n",
        "    for _ in range(N):\n",
        "        decoder_self_attention_block = MultiHeadAttentionBlock(d_model, h, dropout)\n",
        "        decoder_cross_attention_block = MultiHeadAttentionBlock(d_model, h, dropout)\n",
        "        feed_forward_block = FeedForwardBlock(d_model, d_ff, dropout)\n",
        "        decoder_block = DecoderBlock(d_model, decoder_self_attention_block, decoder_cross_attention_block, feed_forward_block, dropout)\n",
        "        decoder_blocks.append(decoder_block)\n",
        "\n",
        "    encoder = Encoder(d_model, nn.ModuleList(encoder_blocks))\n",
        "    decoder = Decoder(d_model, nn.ModuleList(decoder_blocks))\n",
        "\n",
        "    projection_layer = ProjectionLayer(d_model, tgt_vocab_size)\n",
        "\n",
        "    transformer = Transformer(encoder, decoder, src_embed, tgt_embed, src_pos, tgt_pos, projection_layer)\n",
        "\n",
        "    for p in transformer.parameters():\n",
        "        if p.dim() > 1:\n",
        "            nn.init.xavier_uniform_(p)\n",
        "\n",
        "    return transformer"
      ],
      "metadata": {
        "id": "7RV9r0pHB2ej"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_transformer(src_vocab_size: int, tgt_vocab_size: int, src_seq_len: int,\n",
        "                      tgt_seq_len: int, d_model: int=512, N: int=6, h: int=8,\n",
        "                      dropout: float=0.1, d_ff: int=2048) -> Transformer:\n",
        "    src_embed = InputEmbeddings(d_model, src_vocab_size)\n",
        "    tgt_embed = InputEmbeddings(d_model, tgt_vocab_size)\n",
        "\n",
        "    src_pos = PositionalEncoding(d_model, src_seq_len, dropout)\n",
        "    tgt_pos = PositionalEncoding(d_model, tgt_seq_len, dropout)\n",
        "\n",
        "    encoder_blocks = []\n",
        "    for _ in range(N):\n",
        "        encoder_self_attention_block = MultiHeadAttentionBlock(d_model, h, dropout)\n",
        "        feed_forward_block = FeedForwardBlock(d_model, d_ff, dropout)\n",
        "        encoder_block = EncoderBlock(d_model, encoder_self_attention_block, feed_forward_block, dropout)\n",
        "        encoder_blocks.append(encoder_block)\n",
        "\n",
        "    decoder_blocks = []\n",
        "    for _ in range(N):\n",
        "        decoder_self_attention_block = MultiHeadAttentionBlock(d_model, h, dropout)\n",
        "        decoder_cross_attention_block = MultiHeadAttentionBlock(d_model, h, dropout)\n",
        "        feed_forward_block = FeedForwardBlock(d_model, d_ff, dropout)\n",
        "        decoder_block = DecoderBlock(d_model, decoder_self_attention_block, decoder_cross_attention_block, feed_forward_block, dropout)\n",
        "        decoder_blocks.append(decoder_block)\n",
        "\n",
        "    encoder = Encoder(d_model, nn.ModuleList(encoder_blocks))\n",
        "    decoder = Decoder(d_model, nn.ModuleList(decoder_blocks))\n",
        "\n",
        "    projection_layer = ProjectionLayer(d_model, tgt_vocab_size)\n",
        "\n",
        "    transformer = Transformer(encoder, decoder, src_embed, tgt_embed, src_pos, tgt_pos, projection_layer)\n",
        "\n",
        "    for p in transformer.parameters():\n",
        "        if p.dim() > 1:\n",
        "            nn.init.xavier_uniform_(p)\n",
        "\n",
        "    return transformer"
      ],
      "metadata": {
        "id": "9vGS0XLKCPA7"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_or_build_tokenizer(config, ds, lang):\n",
        "    tokenizer_path = Path(config['tokenizer_file'].format(lang))\n",
        "    if not Path.exists(tokenizer_path):\n",
        "        tokenizer = Tokenizer(WordLevel(unk_token=\"[UNK]\"))\n",
        "        tokenizer.pre_tokenizer = Whitespace()\n",
        "        trainer = WordLevelTrainer(special_tokens=[\"[UNK]\", \"[PAD]\", \"[SOS]\", \"[EOS]\"], min_frequency=2)\n",
        "        tokenizer.train_from_iterator(get_all_sentences(ds, lang), trainer=trainer)\n",
        "        tokenizer.save(str(tokenizer_path))\n",
        "    else:\n",
        "        tokenizer = Tokenizer.from_file(str(tokenizer_path))\n",
        "    return tokenizer"
      ],
      "metadata": {
        "id": "x-KGpr5nCWxf"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_all_sentences(ds, lang):\n",
        "    for item in ds:\n",
        "        yield item['translation'][lang]"
      ],
      "metadata": {
        "id": "ayRJcgH8CeRI"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_ds(config):\n",
        "\n",
        "    ds_raw = load_dataset('opus_books', f\"{config['lang_src']}-{config['lang_tgt']}\", split='train')\n",
        "\n",
        "    tokenizer_src = get_or_build_tokenizer(config, ds_raw, config['lang_src'])\n",
        "    tokenizer_tgt = get_or_build_tokenizer(config, ds_raw, config['lang_tgt'])\n",
        "\n",
        "    train_ds_size = int(0.9 * len(ds_raw))\n",
        "    val_ds_size = len(ds_raw) - train_ds_size\n",
        "    train_ds_raw, val_ds_raw = random_split(ds_raw, [train_ds_size, val_ds_size])\n",
        "\n",
        "    train_ds = BilingualDataset(train_ds_raw, tokenizer_src, tokenizer_tgt, config['lang_src'], config['lang_tgt'], config['seq_len'])\n",
        "    val_ds = BilingualDataset(val_ds_raw, tokenizer_src, tokenizer_tgt, config['lang_src'], config['lang_tgt'], config['seq_len'])\n",
        "\n",
        "    max_len_src = 0\n",
        "    max_len_tgt = 0\n",
        "\n",
        "    for item in ds_raw:\n",
        "        src_ids = tokenizer_src.encode(item['translation'][config['lang_src']]).ids\n",
        "        tgt_ids = tokenizer_tgt.encode(item['translation'][config['lang_tgt']]).ids\n",
        "        max_len_src = max(max_len_src, len(src_ids))\n",
        "        max_len_tgt = max(max_len_tgt, len(tgt_ids))\n",
        "\n",
        "    print(f'Max length of source sentence: {max_len_src}')\n",
        "    print(f'Max length of target sentence: {max_len_tgt}')\n",
        "\n",
        "\n",
        "    train_dataloader = DataLoader(train_ds, batch_size=config['batch_size'], shuffle=True)\n",
        "    val_dataloader = DataLoader(val_ds, batch_size=1, shuffle=True)\n",
        "\n",
        "    return train_dataloader, val_dataloader, tokenizer_src, tokenizer_tgt"
      ],
      "metadata": {
        "id": "nUiTU9mqDaEj"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def causal_mask(size):\n",
        "    mask = torch.triu(torch.ones((1, size, size)), diagonal=1).type(torch.int)\n",
        "    return mask == 0"
      ],
      "metadata": {
        "id": "qc91NMOUDc3F"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BilingualDataset(Dataset):\n",
        "    def __init__(self, ds, tokenizer_src, tokenizer_tgt, src_lang, tgt_lang, seq_len):\n",
        "        super().__init__()\n",
        "        self.seq_len = seq_len\n",
        "\n",
        "        self.ds = ds\n",
        "        self.tokenizer_src = tokenizer_src\n",
        "        self.tokenizer_tgt = tokenizer_tgt\n",
        "        self.src_lang = src_lang\n",
        "        self.tgt_lang = tgt_lang\n",
        "\n",
        "        self.sos_token = torch.tensor([tokenizer_tgt.token_to_id(\"[SOS]\")], dtype=torch.int64)\n",
        "        self.eos_token = torch.tensor([tokenizer_tgt.token_to_id(\"[EOS]\")], dtype=torch.int64)\n",
        "        self.pad_token = torch.tensor([tokenizer_tgt.token_to_id(\"[PAD]\")], dtype=torch.int64)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.ds)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        src_target_pair = self.ds[idx]\n",
        "        src_text = src_target_pair['translation'][self.src_lang]\n",
        "        tgt_text = src_target_pair['translation'][self.tgt_lang]\n",
        "\n",
        "        # رد کردن نمونه‌های خالی/نامعتبر\n",
        "        if src_text is None or tgt_text is None:\n",
        "            return None\n",
        "        if src_text.strip() == \"\" or tgt_text.strip() == \"\":\n",
        "            return None\n",
        "\n",
        "        # توکن‌سازی\n",
        "        src_ids_list = self.tokenizer_src.encode(src_text).ids\n",
        "        tgt_ids_list = self.tokenizer_tgt.encode(tgt_text).ids\n",
        "\n",
        "        # اگر بعد از توکنایز خالی شد، رد کن\n",
        "        if len(src_ids_list) == 0 or len(tgt_ids_list) == 0:\n",
        "            return None\n",
        "\n",
        "        src_ids = torch.tensor(src_ids_list, dtype=torch.int64)\n",
        "        tgt_ids = torch.tensor(tgt_ids_list, dtype=torch.int64)\n",
        "\n",
        "        # کوتاه‌سازی برای جا شدن SOS/EOS\n",
        "        if len(src_ids) > self.seq_len - 2:\n",
        "            src_ids = src_ids[: self.seq_len - 2]\n",
        "        if len(tgt_ids) > self.seq_len - 2:\n",
        "            tgt_ids = tgt_ids[: self.seq_len - 2]\n",
        "\n",
        "        # ساخت ورودی‌ها/برچسب‌ها\n",
        "        encoder_input = torch.cat([self.sos_token, src_ids, self.eos_token], dim=0)\n",
        "        decoder_input = torch.cat([self.sos_token, tgt_ids], dim=0)\n",
        "        label         = torch.cat([tgt_ids, self.eos_token], dim=0)\n",
        "\n",
        "        # Padding\n",
        "        encoder_input = self._pad_to_len(encoder_input, self.seq_len)\n",
        "        decoder_input = self._pad_to_len(decoder_input, self.seq_len)\n",
        "        label         = self._pad_to_len(label, self.seq_len)\n",
        "\n",
        "        # اطمینان از اندازه\n",
        "        assert encoder_input.size(0) == self.seq_len\n",
        "        assert decoder_input.size(0) == self.seq_len\n",
        "        assert label.size(0) == self.seq_len\n",
        "\n",
        "        return {\n",
        "            \"encoder_input\": encoder_input,                    # (seq_len)\n",
        "            \"decoder_input\": decoder_input,                    # (seq_len)\n",
        "            \"encoder_mask\": (encoder_input != self.pad_token).unsqueeze(0).unsqueeze(0).int(),  # (1,1,seq_len)\n",
        "            \"decoder_mask\": (decoder_input != self.pad_token).unsqueeze(0).int() & causal_mask(decoder_input.size(0)),  # (1,seq_len,seq_len)\n",
        "            \"label\": label,                                    # (seq_len)\n",
        "            \"src_text\": src_text,\n",
        "            \"tgt_text\": tgt_text,\n",
        "        }\n",
        "\n",
        "    def _pad_to_len(self, tensor, seq_len):\n",
        "        pad_len = seq_len - tensor.size(0)\n",
        "        if pad_len > 0:\n",
        "            pad_tensor = self.pad_token.repeat(pad_len)\n",
        "            tensor = torch.cat([tensor, pad_tensor], dim=0)\n",
        "        else:\n",
        "            tensor = tensor[:seq_len]\n",
        "        return tensor\n",
        "\n",
        "\n",
        "def collate_fn_filter_and_stack(batch):\n",
        "    \"\"\"فیلتر کردن آیتم‌های None و ساخت Batchهای Tensor.\"\"\"\n",
        "    batch = [b for b in batch if b is not None]\n",
        "\n",
        "    # به‌صورت محافظه‌کارانه: اگر کل Batch نامعتبر بود، یک Batchِ صفر-طول برمی‌گردانیم تا حلقه آن را Skip کند.\n",
        "    if len(batch) == 0:\n",
        "        return None\n",
        "\n",
        "    encoder_input = torch.stack([b[\"encoder_input\"] for b in batch], dim=0)\n",
        "    decoder_input = torch.stack([b[\"decoder_input\"] for b in batch], dim=0)\n",
        "    encoder_mask  = torch.stack([b[\"encoder_mask\"]  for b in batch], dim=0)\n",
        "    decoder_mask  = torch.stack([b[\"decoder_mask\"]  for b in batch], dim=0)\n",
        "    label         = torch.stack([b[\"label\"]         for b in batch], dim=0)\n",
        "    src_text      = [b[\"src_text\"] for b in batch]\n",
        "    tgt_text      = [b[\"tgt_text\"] for b in batch]\n",
        "\n",
        "    return {\n",
        "        \"encoder_input\": encoder_input,   # (B, seq_len)\n",
        "        \"decoder_input\": decoder_input,   # (B, seq_len)\n",
        "        \"encoder_mask\":  encoder_mask,    # (B, 1, 1, seq_len)\n",
        "        \"decoder_mask\":  decoder_mask,    # (B, 1, seq_len, seq_len)\n",
        "        \"label\":         label,           # (B, seq_len)\n",
        "        \"src_text\":      src_text,\n",
        "        \"tgt_text\":      tgt_text,\n",
        "    }\n",
        "\n",
        "\n",
        "def get_ds(config):\n",
        "    ds_raw = load_dataset('opus_books', f\"{config['lang_src']}-{config['lang_tgt']}\", split='train')\n",
        "\n",
        "    tokenizer_src = get_or_build_tokenizer(config, ds_raw, config['lang_src'])\n",
        "    tokenizer_tgt = get_or_build_tokenizer(config, ds_raw, config['lang_tgt'])\n",
        "\n",
        "    train_ds_size = int(0.9 * len(ds_raw))\n",
        "    val_ds_size   = len(ds_raw) - train_ds_size\n",
        "    train_ds_raw, val_ds_raw = random_split(ds_raw, [train_ds_size, val_ds_size])\n",
        "\n",
        "    train_ds = BilingualDataset(train_ds_raw, tokenizer_src, tokenizer_tgt,\n",
        "                                config['lang_src'], config['lang_tgt'], config['seq_len'])\n",
        "    val_ds   = BilingualDataset(val_ds_raw,   tokenizer_src, tokenizer_tgt,\n",
        "                                config['lang_src'], config['lang_tgt'], config['seq_len'])\n",
        "\n",
        "    # اطلاع از بیشینه طول‌ها (صرفاً پرینت)\n",
        "    max_len_src = 0\n",
        "    max_len_tgt = 0\n",
        "    for item in ds_raw:\n",
        "        src_ids = tokenizer_src.encode(item['translation'][config['lang_src']]).ids\n",
        "        tgt_ids = tokenizer_tgt.encode(item['translation'][config['lang_tgt']]).ids\n",
        "        max_len_src = max(max_len_src, len(src_ids))\n",
        "        max_len_tgt = max(max_len_tgt, len(tgt_ids))\n",
        "    print(f\"Max length of source sentence: {max_len_src}\")\n",
        "    print(f\"Max length of target sentence: {max_len_tgt}\")\n",
        "\n",
        "    # DataLoaderها با collate_fn سفارشی\n",
        "    train_dataloader = DataLoader(\n",
        "        train_ds,\n",
        "        batch_size=config['batch_size'],\n",
        "        shuffle=True,\n",
        "        drop_last=True,                      # برای جلوگیری از Batch خالی\n",
        "        collate_fn=collate_fn_filter_and_stack,\n",
        "        num_workers=0,                       # در Colab معمولاً 0 مطمئن‌تره\n",
        "        pin_memory=torch.cuda.is_available()\n",
        "    )\n",
        "\n",
        "    val_dataloader = DataLoader(\n",
        "        val_ds,\n",
        "        batch_size=1,\n",
        "        shuffle=True,\n",
        "        drop_last=False,\n",
        "        collate_fn=collate_fn_filter_and_stack,\n",
        "        num_workers=0,\n",
        "        pin_memory=torch.cuda.is_available()\n",
        "    )\n",
        "\n",
        "    return train_dataloader, val_dataloader, tokenizer_src, tokenizer_tgt\n"
      ],
      "metadata": {
        "id": "T1KDNnIeDriT"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def greedy_decode(model, source, source_mask, tokenizer_src, tokenizer_tgt, max_len, device):\n",
        "    sos_idx = tokenizer_tgt.token_to_id(\"[SOS]\")\n",
        "    eos_idx = tokenizer_tgt.token_to_id(\"[EOS]\")\n",
        "\n",
        "    # 1️⃣ مرحله‌ی encode\n",
        "    encoder_output = model.encode(source, source_mask)\n",
        "\n",
        "    # 2️⃣ مقداردهی اولیه‌ی ورودی دیکودر با توکن شروع جمله\n",
        "    decoder_input = torch.empty(1, 1).fill_(sos_idx).type_as(source).to(device)\n",
        "\n",
        "    # 3️⃣ تولید توکن‌ها به صورت تکراری تا رسیدن به حداکثر طول یا توکن [EOS]\n",
        "    for _ in range(max_len - 1):\n",
        "        # ایجاد ماسک علی (causal mask)\n",
        "        tgt_mask = causal_mask(decoder_input.size(1)).type_as(source_mask)\n",
        "\n",
        "        # عبور از دیکودر\n",
        "        decoder_output = model.decode(encoder_output, source_mask, decoder_input, tgt_mask)\n",
        "\n",
        "        # پیش‌بینی توکن بعدی\n",
        "        proj_output = model.project(decoder_output[:, -1])  # فقط آخرین گام زمانی\n",
        "        next_word = torch.argmax(proj_output, dim=-1).item()\n",
        "\n",
        "        # افزودن توکن پیش‌بینی‌شده به ورودی دیکودر\n",
        "        next_token = torch.tensor([[next_word]], dtype=source.dtype).to(device)\n",
        "        decoder_input = torch.cat([decoder_input, next_token], dim=1)\n",
        "\n",
        "        # شرط پایان\n",
        "        if next_word == eos_idx:\n",
        "            break\n",
        "\n",
        "    return decoder_input.squeeze(0)\n"
      ],
      "metadata": {
        "id": "myntfkY_EBLS"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_validation(model, validation_ds, tokenizer_src, tokenizer_tgt, max_len, device, print_msg, global_step, writer, num_examples=2):\n",
        "    model.eval()\n",
        "    count = 0\n",
        "\n",
        "    source_texts = []\n",
        "    expected = []\n",
        "    predicted = []\n",
        "\n",
        "    try:\n",
        "        with os.popen('stty size', 'r') as console:\n",
        "            _, console_width = console.read().split()\n",
        "            console_width = int(console_width)\n",
        "    except:\n",
        "        console_width = 80\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in validation_ds:\n",
        "            count += 1\n",
        "            encoder_input = batch[\"encoder_input\"].to(device) # (b, seq_len)\n",
        "            encoder_mask = batch[\"encoder_mask\"].to(device) # (b, 1, 1, seq_len)\n",
        "\n",
        "            assert encoder_input.size(\n",
        "                0) == 1, \"Batch size must be 1 for validation\"\n",
        "\n",
        "            model_out = greedy_decode(model, encoder_input, encoder_mask, tokenizer_src, tokenizer_tgt, max_len, device)\n",
        "\n",
        "            source_text = batch[\"src_text\"][0]\n",
        "            target_text = batch[\"tgt_text\"][0]\n",
        "            model_out_text = tokenizer_tgt.decode(model_out.detach().cpu().numpy())\n",
        "\n",
        "            source_texts.append(source_text)\n",
        "            expected.append(target_text)\n",
        "            predicted.append(model_out_text)\n",
        "\n",
        "\n",
        "            print_msg('-'*console_width)\n",
        "            print_msg(f\"{f'SOURCE: ':>12}{source_text}\")\n",
        "            print_msg(f\"{f'TARGET: ':>12}{target_text}\")\n",
        "            print_msg(f\"{f'PREDICTED: ':>12}{model_out_text}\")\n",
        "\n",
        "            if count == num_examples:\n",
        "                print_msg('-'*console_width)\n",
        "                break\n",
        "\n",
        "    if writer:\n",
        "        metric = torchmetrics.CharErrorRate()\n",
        "        cer = metric(predicted, expected)\n",
        "        writer.add_scalar('validation cer', cer, global_step)\n",
        "        writer.flush()\n",
        "\n",
        "        metric = torchmetrics.WordErrorRate()\n",
        "        wer = metric(predicted, expected)\n",
        "        writer.add_scalar('validation wer', wer, global_step)\n",
        "        writer.flush()\n",
        "\n",
        "        # Compute the BLEU metric\n",
        "        metric = torchmetrics.BLEUScore()\n",
        "        bleu = metric(predicted, expected)\n",
        "        writer.add_scalar('validation BLEU', bleu, global_step)\n",
        "        writer.flush()"
      ],
      "metadata": {
        "id": "lfzpW-L1EV4x"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_model(config, vocab_src_len, vocab_tgt_len):\n",
        "    model = build_transformer(vocab_src_len, vocab_tgt_len, config[\"seq_len\"], config['seq_len'], d_model=config['d_model'])\n",
        "    return model"
      ],
      "metadata": {
        "id": "UKTnynqdEYZF"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_config():\n",
        "    return {\n",
        "        \"batch_size\": 8,\n",
        "        \"num_epochs\": 8,\n",
        "        \"lr\": 10**-4,\n",
        "        \"seq_len\": 350,\n",
        "        \"d_model\": 512,\n",
        "        \"datasource\": 'opus_books',\n",
        "        \"lang_src\": \"en\",\n",
        "        \"lang_tgt\": \"it\",\n",
        "        \"model_folder\": \"weights\",\n",
        "        \"model_basename\": \"tmodel_\",\n",
        "        \"preload\": \"latest\",\n",
        "        \"tokenizer_file\": \"tokenizer_{0}.json\",\n",
        "        \"experiment_name\": \"runs/tmodel\"\n",
        "    }\n",
        "\n",
        "def get_weights_file_path(config, epoch: str):\n",
        "    model_folder = f\"{config['datasource']}_{config['model_folder']}\"\n",
        "    model_filename = f\"{config['model_basename']}{epoch}.pt\"\n",
        "    return str(Path('.') / model_folder / model_filename)\n",
        "\n",
        "# Find the latest weights file in the weights folder\n",
        "def latest_weights_file_path(config):\n",
        "    model_folder = f\"{config['datasource']}_{config['model_folder']}\"\n",
        "    model_filename = f\"{config['model_basename']}*\"\n",
        "    weights_files = list(Path(model_folder).glob(model_filename))\n",
        "    if len(weights_files) == 0:\n",
        "        return None\n",
        "    weights_files.sort()\n",
        "    return str(weights_files[-1])"
      ],
      "metadata": {
        "id": "7lOyt43sEb4A"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(config):\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.has_mps or torch.backends.mps.is_available() else \"cpu\"\n",
        "    print(\"Using device:\", device)\n",
        "    if (device == 'cuda'):\n",
        "        print(f\"Device name: {torch.cuda.get_device_name(device.index)}\")\n",
        "        print(f\"Device memory: {torch.cuda.get_device_properties(device.index).total_memory / 1024 ** 3} GB\")\n",
        "    elif (device == 'mps'):\n",
        "        print(f\"Device name: <mps>\")\n",
        "    else:\n",
        "        print(\"NOTE: If you have a GPU, consider using it for training.\")\n",
        "        print(\"      On a Windows machine with NVidia GPU, check this video: https://www.youtube.com/watch?v=GMSjDTU8Zlc\")\n",
        "        print(\"      On a Mac machine, run: pip3 install --pre torch torchvision torchaudio torchtext --index-url https://download.pytorch.org/whl/nightly/cpu\")\n",
        "    device = torch.device(device)\n",
        "\n",
        "    Path(f\"{config['datasource']}_{config['model_folder']}\").mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    train_dataloader, val_dataloader, tokenizer_src, tokenizer_tgt = get_ds(config)\n",
        "    model = get_model(config, tokenizer_src.get_vocab_size(), tokenizer_tgt.get_vocab_size()).to(device)\n",
        "    # Tensorboard\n",
        "    writer = SummaryWriter(config['experiment_name'])\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=config['lr'], eps=1e-9)\n",
        "\n",
        "    initial_epoch = 0\n",
        "    global_step = 0\n",
        "    preload = config['preload']\n",
        "    model_filename = latest_weights_file_path(config) if preload == 'latest' else get_weights_file_path(config, preload) if preload else None\n",
        "    if model_filename:\n",
        "        print(f'Preloading model {model_filename}')\n",
        "        state = torch.load(model_filename)\n",
        "        model.load_state_dict(state['model_state_dict'])\n",
        "        initial_epoch = state['epoch'] + 1\n",
        "        optimizer.load_state_dict(state['optimizer_state_dict'])\n",
        "        global_step = state['global_step']\n",
        "    else:\n",
        "        print('No model to preload, starting from scratch')\n",
        "\n",
        "    loss_fn = nn.CrossEntropyLoss(ignore_index=tokenizer_src.token_to_id('[PAD]'), label_smoothing=0.1).to(device)\n",
        "\n",
        "    for epoch in range(initial_epoch, config['num_epochs']):\n",
        "        torch.cuda.empty_cache()\n",
        "        model.train()\n",
        "        batch_iterator = tqdm(train_dataloader, desc=f\"Processing Epoch {epoch:02d}\")\n",
        "        for batch in batch_iterator:\n",
        "\n",
        "            encoder_input = batch['encoder_input'].to(device) # (b, seq_len)\n",
        "            decoder_input = batch['decoder_input'].to(device) # (B, seq_len)\n",
        "            encoder_mask = batch['encoder_mask'].to(device) # (B, 1, 1, seq_len)\n",
        "            decoder_mask = batch['decoder_mask'].to(device) # (B, 1, seq_len, seq_len)\n",
        "\n",
        "            encoder_output = model.encode(encoder_input, encoder_mask) # (B, seq_len, d_model)\n",
        "            decoder_output = model.decode(encoder_output, encoder_mask, decoder_input, decoder_mask) # (B, seq_len, d_model)\n",
        "            proj_output = model.project(decoder_output) # (B, seq_len, vocab_size)\n",
        "\n",
        "            label = batch['label'].to(device) # (B, seq_len)\n",
        "\n",
        "            loss = loss_fn(proj_output.view(-1, tokenizer_tgt.get_vocab_size()), label.view(-1))\n",
        "            batch_iterator.set_postfix({\"loss\": f\"{loss.item():6.3f}\"})\n",
        "\n",
        "            writer.add_scalar('train loss', loss.item(), global_step)\n",
        "            writer.flush()\n",
        "\n",
        "            loss.backward()\n",
        "\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "            global_step += 1\n",
        "\n",
        "        run_validation(model, val_dataloader, tokenizer_src, tokenizer_tgt, config['seq_len'], device, lambda msg: batch_iterator.write(msg), global_step, writer)\n",
        "\n",
        "        model_filename = get_weights_file_path(config, f\"{epoch:02d}\")\n",
        "        torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'global_step': global_step\n",
        "        }, model_filename)"
      ],
      "metadata": {
        "id": "lGb_gNhYEngz"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "    warnings.filterwarnings(\"ignore\")\n",
        "    config = get_config()\n",
        "    train_model(config)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PcZ8ssS6Es0R",
        "outputId": "034dde89-d9d6-4934-de6d-4dcdab180b44"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Device name: Tesla T4\n",
            "Device memory: 14.74127197265625 GB\n",
            "Max length of source sentence: 309\n",
            "Max length of target sentence: 274\n",
            "No model to preload, starting from scratch\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Epoch 00: 100%|██████████| 3637/3637 [24:53<00:00,  2.43it/s, loss=5.893]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "    SOURCE: The District Marshals carried little plates filled with ballot balls from their own tables to the Provincial table, and the election began.\n",
            "    TARGET: I marescialli distrettuali passavano, con dei vassoi in cui erano le palle, dalle proprie tavole a quella del governatorato e le elezioni cominciarono.\n",
            " PREDICTED: Il signor Rochester era un ’ altra , e il suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo\n",
            "--------------------------------------------------------------------------------\n",
            "    SOURCE: I groped on. Again a whitish object gleamed before me: it was a gate--a wicket; it moved on its hinges as I touched it.\n",
            "    TARGET: A un tratto vidi qualcosa di chiaro davanti a me: era un cancello con un saliscendi.\n",
            " PREDICTED: Mi , e mi , e mi , e che mi .\n",
            "--------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Epoch 01: 100%|██████████| 3637/3637 [24:58<00:00,  2.43it/s, loss=5.605]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "    SOURCE: 'It's a pun!' the King added in an offended tone, and everybody laughed, 'Let the jury consider their verdict,' the King said, for about the twentieth time that day.\n",
            "    TARGET: È un bisticcio — soggiunse il Re con voce irata, e tutti allora risero. — Che i giurati ponderino il loro verdetto — ripetè il Re, forse per la ventesima volta quel giorno.\n",
            " PREDICTED: — È un ’ altra , — disse il Cappellaio , e , con un ’ altra volta , e , la sua voce , con un ’ altra .\n",
            "--------------------------------------------------------------------------------\n",
            "    SOURCE: That life was revealed by religion, but a religion that had nothing in common with that which Kitty had known since her childhood and which found expression in Mass and vespers at the private chapel of the Widows' Almshouse where one could meet one's friends, and in learning Slavonic texts by heart with the priest. This was a lofty, mystical religion connected with a series of beautiful thoughts and feelings, which it was not only possible to believe because one was told to, but even to love.\n",
            "    TARGET: Questa vita era rivelata dalla religione, ma da una religione che non aveva nulla di comune con quella che Kitty praticava dall’infanzia e che tutta si esprimeva ed esauriva nell’assistere alla messa e ai vespri, nel recarsi alla «Casa delle vedove» dove si potevano incontrare dei conoscenti, e nello studiare a memoria col batjuška testi in slavo antico: quest’altra era una religione altissima, misteriosa, legata a una serie di pensieri e di sentimenti splendidi, in cui non solo si poteva credere, perché così era comandato, ma che si poteva amare.\n",
            " PREDICTED: Ma non era mai mai mai mai più , ma non aveva mai mai mai mai mai mai , e , che non aveva mai mai mai mai mai mai , e che non aveva mai mai mai mai mai mai mai , e che non aveva mai mai mai mai mai mai mai mai mai mai mai , e che non aveva mai mai mai mai mai mai mai mai mai , ma che non poteva essere mai mai mai mai mai mai mai mai mai mai , ma non poteva essere mai mai mai mai mai mai mai mai mai mai mai , e non poteva essere mai , ma che non poteva essere mai mai mai , e non poteva essere mai mai mai mai mai , e che non poteva essere mai mai mai mai mai mai mai mai mai mai mai mai mai mai mai mai mai mai mai mai mai mai mai mai mai mai mai mai mai mai mai mai mai mai mai mai mai mai mai mai mai mai mai mai mai mai mai mai mai mai mai mai mai mai mai mai mai mai mai mai mai mai mai mai mai mai mai mai mai mai mai mai mai mai mai mai mai mai mai mai mai mai mai mai mai mai mai mai mai mai mai mai mai mai mai , ma non poteva essere mai mai mai mai mai , ma non poteva essere mai mai mai mai , e che non solo , ma non , e non , e non , e non , ma , e non , ma che non , ma non poteva essere mai , ma che non poteva essere mai , ma non poteva essere mai , ma non poteva essere mai , ma non poteva essere mai , e non poteva essere mai , ma non poteva essere mai mai mai mai , e non poteva essere mai , ma non poteva essere mai , ma non\n",
            "--------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Epoch 02: 100%|██████████| 3637/3637 [24:57<00:00,  2.43it/s, loss=5.438]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "    SOURCE: \"You may.\"\n",
            "    TARGET: — Sì, Jane.\n",
            " PREDICTED: — Voi siete .\n",
            "--------------------------------------------------------------------------------\n",
            "    SOURCE: I have dismissed, with the fee of an orange, the little orphan who serves me as a handmaid. I am sitting alone on the hearth.\n",
            "    TARGET: Avevo mandata a casa la piccola orfana che mi serviva, regalandole un arancio, e stavo seduta sola accanto al focolare.\n",
            " PREDICTED: Mi ho detto che il suo , e che mi , e mi , e mi il mio .\n",
            "--------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Epoch 03:  10%|█         | 364/3637 [02:30<22:26,  2.43it/s, loss=5.019]"
          ]
        }
      ]
    }
  ]
}